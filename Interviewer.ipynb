{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6fc320",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "526dbfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access your Google API Key\n",
    "# Make sure your .env file has GOOGLE_API_KEY=YOUR_API_KEY\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not GOOGLE_API_KEY:\n",
    "    raise ValueError(\"Google API Key not found. Please set it in your .env file.\")\n",
    "\n",
    "# Set the environment variable for LangChain and google-generativeai\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
    "\n",
    "print(\"API Key loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a00b989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Imports\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "# Other useful imports\n",
    "import json # For handling structured data like our question bank\n",
    "from typing import List, Dict, Any # For type hinting, improves code readability and maintainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f221c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini LLM initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# 2.1 Initialize the Google Gemini LLM \n",
    "\n",
    "# Initialize the ChatGoogleGenerativeAI model\n",
    "# Using 'models/gemini-1.5-flash-latest' as per  preference.\n",
    "llm = ChatGoogleGenerativeAI(model=\"models/gemini-2.0-flash-lite\", temperature=0.2) # Changed model name\n",
    "\n",
    "print(\"Gemini LLM initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8be30499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain Conversation Memory initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Parth\\AppData\\Local\\Temp\\ipykernel_3020\\2795900466.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n"
     ]
    }
   ],
   "source": [
    "# 2.2 Set up LangChain Memory for Conversation\n",
    "\n",
    "# Initialize ConversationBufferMemory to store the conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"history\", return_messages=True)\n",
    "\n",
    "print(\"LangChain Conversation Memory initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5141de51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 Excel questions from excel_questions.json\n"
     ]
    }
   ],
   "source": [
    "# 3. Excel Question Bank Definition (Loaded from JSON)\n",
    "\n",
    "import json\n",
    "\n",
    "def load_questions_from_json(file_path: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Loads Excel interview questions from a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            questions = json.load(f)\n",
    "        print(f\"Loaded {len(questions)} Excel questions from {file_path}\")\n",
    "        return questions\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Question bank file '{file_path}' not found. Please create it.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: Could not decode JSON from '{file_path}'. Check file format.\")\n",
    "        return []\n",
    "\n",
    "# Define the path to your JSON file\n",
    "QUESTIONS_FILE_PATH = 'excel_questions.json'\n",
    "\n",
    "# Load the questions\n",
    "excel_questions = load_questions_from_json(QUESTIONS_FILE_PATH)\n",
    "\n",
    "#optionally print one question to verify the structure\n",
    "#print(json.dumps(excel_questions[0], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9effba9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExcelEvaluator class defined.\n",
      "\n",
      "==================================================\n",
      "--- Testing Good Answer ---\n",
      "==================================================\n",
      "\n",
      "### Evaluation Result for Good Answer:\n",
      "**Score:** 5 / 5\n",
      "**Justification:** The candidate demonstrates a clear and accurate understanding of both relative and absolute cell references, including their behavior when copied. The explanation is precise and easy to understand. The examples provided are appropriate and relevant to real-world scenarios. Furthermore, the candidate goes above and beyond by explaining mixed references (A$1 and $A1), showcasing a deeper understanding of Excel's capabilities. This comprehensive response exceeds the expectations of the question.\n",
      "\n",
      "\n",
      "==================================================\n",
      "--- Testing Poor Answer ---\n",
      "==================================================\n",
      "\n",
      "### Evaluation Result for Poor Answer:\n",
      "**Score:** 1 / 5\n",
      "**Justification:** The candidate correctly identifies the syntax for relative and absolute cell references. However, the answer demonstrates a complete lack of understanding of their behavior when copied and provides no examples of when to use each. This falls far short of the expected answer, which requires a clear explanation of the core difference (how they change when copied) and practical examples.\n",
      "\n",
      "\n",
      "==================================================\n",
      "--- Testing Average Answer ---\n",
      "==================================================\n",
      "\n",
      "### Evaluation Result for Average Answer:\n",
      "**Score:** 4 / 5\n",
      "**Justification:** The candidate demonstrates a clear understanding of the core difference between relative and absolute references, correctly identifying their behavior when copied ('moves when dragged' vs. 'stays put'). The examples provided are also appropriate and relevant to real-world scenarios. The answer is concise and accurate, although it could benefit from a slightly more detailed explanation of *how* the relative reference changes (e.g., incrementing row/column numbers) and a more specific example of a series (e.g., calculating a sum across a range). Overall, the answer is strong and meets the core requirements.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- STEP 4.1: ExcelEvaluator Class ---\n",
    "\n",
    "import json\n",
    "import re\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, MessagesPlaceholder # Corrected imports\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "class ExcelEvaluator:\n",
    "    def __init__(self, llm: ChatGoogleGenerativeAI):\n",
    "        self.llm = llm\n",
    "        # Define the prompt template for evaluating an Excel answer\n",
    "        self.evaluation_template = PromptTemplate(\n",
    "            input_variables=[\n",
    "                \"excel_question\",\n",
    "                \"expected_answer_description\",\n",
    "                \"evaluation_criteria\",\n",
    "                \"candidate_answer\"\n",
    "            ],\n",
    "            template=\"\"\"You are an expert Excel interviewer and a highly precise evaluator.\n",
    "Your task is to critically assess a candidate's answer to an Excel question.\n",
    "\n",
    "Here's the Excel question asked:\n",
    "\"{excel_question}\"\n",
    "\n",
    "Here is the detailed expected correct answer/description from an expert:\n",
    "\"{expected_answer_description}\"\n",
    "\n",
    "Here are the specific criteria you must use for evaluation:\n",
    "\"{evaluation_criteria}\"\n",
    "\n",
    "Here is the candidate's answer:\n",
    "\"{candidate_answer}\"\n",
    "\n",
    "Based on the above, provide a score from 1 to 5 (where 1 is 'Poor', 3 is 'Adequate', and 5 is 'Excellent').\n",
    "Then, provide a brief, objective justification for your score, highlighting strengths and weaknesses compared to the expected answer and criteria.\n",
    "\n",
    "Format your response STRICTLY as a JSON object, without any additional text or markdown formatting (like ```json) outside of the JSON itself.\n",
    "The JSON object MUST have the following two keys: \"score\" (integer from 1-5) and \"justification\" (string).\n",
    "Example good output: {{\"score\": 4, \"justification\": \"Good answer.\"}}\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    def evaluate_answer(\n",
    "        self,\n",
    "        excel_question: str,\n",
    "        expected_answer_description: str,\n",
    "        evaluation_criteria: str,\n",
    "        candidate_answer: str\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Evaluates a candidate's answer to an Excel question using the LLM.\n",
    "\n",
    "        Args:\n",
    "            excel_question (str): The original Excel question.\n",
    "            expected_answer_description (str): A detailed description of the correct answer.\n",
    "            evaluation_criteria (str): Specific criteria for the LLM to use during evaluation.\n",
    "            candidate_answer (str): The candidate's response.\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, Any]: A dictionary containing the 'score' (int) and 'justification' (str).\n",
    "                            Returns an error structure if parsing fails.\n",
    "        \"\"\"\n",
    "        response_text = \"\"\n",
    "        json_string_to_parse = \"\"\n",
    "\n",
    "        try:\n",
    "            # Create a runnable sequence: evaluation_template -> llm\n",
    "            evaluation_chain = self.evaluation_template | self.llm\n",
    "\n",
    "            # Generate the evaluation using the LLM\n",
    "            response_text = evaluation_chain.invoke(\n",
    "                {\n",
    "                    \"excel_question\": excel_question,\n",
    "                    \"expected_answer_description\": expected_answer_description,\n",
    "                    \"evaluation_criteria\": evaluation_criteria,\n",
    "                    \"candidate_answer\": candidate_answer\n",
    "                }\n",
    "            )\n",
    "\n",
    "            # If the LLM returns an AIMessage, extract the content\n",
    "            if hasattr(response_text, 'content'):\n",
    "                response_text = response_text.content\n",
    "\n",
    "            # Robust JSON extraction\n",
    "            match = re.search(r\"```json\\n(.*?)```\", response_text, re.DOTALL)\n",
    "            if match:\n",
    "                json_string_to_parse = match.group(1).strip()\n",
    "            else:\n",
    "                json_string_to_parse = response_text.strip()\n",
    "\n",
    "            evaluation_result = json.loads(json_string_to_parse)\n",
    "\n",
    "            # Basic validation of the parsed result\n",
    "            if \"score\" not in evaluation_result or \"justification\" not in evaluation_result:\n",
    "                raise ValueError(\"LLM response missing 'score' or 'justification' key.\")\n",
    "            if not isinstance(evaluation_result[\"score\"], int) or not (1 <= evaluation_result[\"score\"] <= 5):\n",
    "                 raise ValueError(\"LLM score is not an integer between 1 and 5.\")\n",
    "\n",
    "            return evaluation_result\n",
    "\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"--- JSON Parsing Error ---\")\n",
    "            print(f\"Error: Could not decode JSON from LLM response. Details: {e}\")\n",
    "            print(f\"Attempted to parse: `{json_string_to_parse}`\")\n",
    "            # Corrected f-string syntax here\n",
    "            print(f\"Original LLM Response: ```{response_text}```\")\n",
    "            print(f\"--------------------------\")\n",
    "            return {\"score\": 0, \"justification\": f\"Evaluation error: LLM response not valid JSON. {e}\"}\n",
    "        except ValueError as e:\n",
    "            print(f\"--- Validation Error ---\")\n",
    "            print(f\"Error: LLM response parsing/validation failed. Details: {e}\")\n",
    "            print(f\"Attempted to parse: `{json_string_to_parse}`\")\n",
    "            print(f\"Original LLM Response: ```{response_text}```\")\n",
    "            print(f\"--------------------------\")\n",
    "            return {\"score\": 0, \"justification\": f\"Evaluation error: LLM response format invalid. {e}\"}\n",
    "        except Exception as e:\n",
    "            print(f\"--- Unexpected Error ---\")\n",
    "            print(f\"An unexpected error occurred during evaluation: {e}\")\n",
    "            print(f\"Original LLM Response (if available): ```{response_text}```\")\n",
    "            print(f\"--------------------------\")\n",
    "            return {\"score\": 0, \"justification\": f\"Unexpected evaluation error: {e}\"}\n",
    "\n",
    "\n",
    "print(\"ExcelEvaluator class defined.\")\n",
    "\n",
    "# --- Testing the ExcelEvaluator (Ensure llm and excel_questions are available) ---\n",
    "if 'llm' not in locals() or 'excel_questions' not in locals() or not excel_questions:\n",
    "    print(\"\\nWARNING: 'llm' or 'excel_questions' not found. Please run Step 1, 2, and 3 cells first.\")\n",
    "    print(\"Skipping ExcelEvaluator tests.\")\n",
    "else:\n",
    "    evaluator = ExcelEvaluator(llm)\n",
    "    test_question = excel_questions[0] # Use the first question from your loaded bank\n",
    "\n",
    "    test_candidate_answer_good = \"\"\"\n",
    "A relative cell reference, like `A1`, changes both its row and column parts when the formula is copied or dragged to other cells. For instance, if `=A1+B1` is in `C1` and copied to `C2`, it automatically becomes `=A2+B2`. This is incredibly useful when you want to apply the same calculation logic across a range of data, such as summing values in a column or row.\n",
    "\n",
    "An absolute cell reference, denoted by dollar signs (e.g., `$A$1`), ensures that both the row and column parts of the cell reference remain fixed, or 'locked', when the formula is copied. This is vital when you need a formula to consistently refer back to a single, specific cell, like a fixed exchange rate, a tax percentage, or a lookup value from a master table, regardless of where the formula is placed.\n",
    "\n",
    "Furthermore, there are **partial absolute references**:\n",
    "* `A$1` (mixed reference): The column (A) changes when copied across columns, but the row (1) remains fixed when copied down rows. Useful for applying a formula across a row where a fixed header row is involved.\n",
    "* `$A1` (mixed reference): The column (A) remains fixed when copied across columns, but the row (1) changes when copied down rows. Useful for applying a formula down a column where a fixed lookup column is involved.\n",
    "\n",
    "These partial references provide immense flexibility for complex calculations across grids of data.\n",
    "\"\"\"\n",
    "    test_candidate_answer_poor = \"Relative is A1, absolute is $A$1. Don't know when to use them.\"\n",
    "    test_candidate_answer_average = \"Relative is like A1, it moves when dragged. Absolute is $A$1, it stays put. Use relative for series, absolute for fixed lookups like a discount percentage.\"\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Testing Good Answer ---\")\n",
    "    print(\"=\"*50)\n",
    "    good_evaluation = evaluator.evaluate_answer(\n",
    "        excel_question=test_question[\"question\"],\n",
    "        expected_answer_description=test_question[\"expected_answer_description\"],\n",
    "        evaluation_criteria=test_question[\"evaluation_criteria\"],\n",
    "        candidate_answer=test_candidate_answer_good\n",
    "    )\n",
    "\n",
    "    if good_evaluation and good_evaluation[\"score\"] != 0:\n",
    "        print(\"\\n### Evaluation Result for Good Answer:\")\n",
    "        print(f\"**Score:** {good_evaluation['score']} / 5\")\n",
    "        print(f\"**Justification:** {good_evaluation['justification']}\\n\")\n",
    "    else:\n",
    "        print(f\"\\nEvaluation failed for Good Answer. Details: {good_evaluation['justification']}\\n\")\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Testing Poor Answer ---\")\n",
    "    print(\"=\"*50)\n",
    "    poor_evaluation = evaluator.evaluate_answer(\n",
    "        excel_question=test_question[\"question\"],\n",
    "        expected_answer_description=test_question[\"expected_answer_description\"],\n",
    "        evaluation_criteria=test_question[\"evaluation_criteria\"],\n",
    "        candidate_answer=test_candidate_answer_poor\n",
    "    )\n",
    "\n",
    "    if poor_evaluation and poor_evaluation[\"score\"] != 0:\n",
    "        print(\"\\n### Evaluation Result for Poor Answer:\")\n",
    "        print(f\"**Score:** {poor_evaluation['score']} / 5\")\n",
    "        print(f\"**Justification:** {poor_evaluation['justification']}\\n\")\n",
    "    else:\n",
    "        print(f\"\\nEvaluation failed for Poor Answer. Details: {poor_evaluation['justification']}\\n\")\n",
    "\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Testing Average Answer ---\")\n",
    "    print(\"=\"*50)\n",
    "    average_evaluation = evaluator.evaluate_answer(\n",
    "        excel_question=test_question[\"question\"],\n",
    "        expected_answer_description=test_question[\"expected_answer_description\"],\n",
    "        evaluation_criteria=test_question[\"evaluation_criteria\"],\n",
    "        candidate_answer=test_candidate_answer_average\n",
    "    )\n",
    "\n",
    "    if average_evaluation and average_evaluation[\"score\"] != 0:\n",
    "        print(\"\\n### Evaluation Result for Average Answer:\")\n",
    "        print(f\"**Score:** {average_evaluation['score']} / 5\")\n",
    "        print(f\"**Justification:** {average_evaluation['justification']}\\n\")\n",
    "    else:\n",
    "        print(f\"\\nEvaluation failed for Average Answer. Details: {average_evaluation['justification']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d12c0f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedbackGenerator class defined.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if \\'llm\\' not in locals():\\n    print(\"\\nWARNING: \\'llm\\' not found. Please run Step 2.1 cell first.\")\\n    print(\"Skipping FeedbackGenerator tests.\")\\nelse:\\n    feedback_gen = FeedbackGenerator(llm)\\n\\n    # Create SAMPLE interview results to test the FeedbackGenerator\\n    # This simulates the output we\\'d get from the ExcelEvaluator for multiple questions\\n    sample_interview_results = [\\n        {\\n            \"id\": \"EXL001\",\\n            \"topic\": \"Formulas & Functions\",\\n            \"question\": \"Explain relative vs absolute cell references...\",\\n            \"candidate_answer\": \"Relative moves, absolute stays.\",\\n            \"evaluation\": {\"score\": 5, \"justification\": \"Candidate provided a perfectly comprehensive explanation, including partial absolute references and clear use cases. Demonstrated excellent understanding.\"}\\n        },\\n        {\\n            \"id\": \"EXL002\",\\n            \"topic\": \"Data Analysis & Pivot Tables\",\\n            \"question\": \"Describe steps for PivotTable and purpose of Values area.\",\\n            \"candidate_answer\": \"Click insert, pivot table. Values is for sums.\",\\n            \"evaluation\": {\"score\": 3, \"justification\": \"Candidate understood the basic steps for PivotTable creation and the high-level purpose of the Values area. However, the explanation lacked detail on specific steps and various aggregation functions beyond sums, indicating an adequate but not comprehensive understanding.\"}\\n        },\\n        {\\n            \"id\": \"EXL003\",\\n            \"topic\": \"Conditional Formatting\",\\n            \"question\": \"How to highlight sales above average?\",\\n            \"candidate_answer\": \"Select data, home tab, conditional formatting, above average rule.\",\\n            \"evaluation\": {\"score\": 4, \"justification\": \"Candidate provided accurate and concise steps for applying \\'Above Average\\' conditional formatting. Demonstrated practical knowledge. A minor detail missing was mention of selecting the formatting style, but otherwise solid.\"}\\n        },\\n        {\\n            \"id\": \"EXL004\",\\n            \"topic\": \"VLOOKUP/XLOOKUP\",\\n            \"question\": \"Explain VLOOKUP scenario, limitations, and XLOOKUP alternative.\",\\n            \"candidate_answer\": \"VLOOKUP looks left to right. XLOOKUP is better.\",\\n            \"evaluation\": {\"score\": 2, \"justification\": \"Candidate identified one key limitation of VLOOKUP (left-to-right) and correctly named XLOOKUP as an alternative. However, the explanation of a VLOOKUP scenario was absent, and the description of XLOOKUP\\'s advantages was extremely brief, missing most crucial improvements. Demonstrates weak understanding.\"}\\n        }\\n    ]\\n\\n    print(\"\\n\" + \"=\"*50)\\n    print(\"--- Generating Sample Feedback Report ---\")\\n    print(\"=\"*50)\\n    sample_report = feedback_gen.generate_feedback_report(sample_interview_results)\\n    print(sample_report)\\n    print(\"=\"*50 + \"\\n\")'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- STEP 4.2: FeedbackGenerator Class ---\n",
    "\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI \n",
    "\n",
    "\n",
    "class FeedbackGenerator:\n",
    "    def __init__(self, llm: ChatGoogleGenerativeAI):\n",
    "        self.llm = llm\n",
    "        self.feedback_template = PromptTemplate(\n",
    "            input_variables=[\"interview_results\"],\n",
    "            template=\"\"\"You are an HR professional and an Excel expert.\n",
    "You have the evaluation results from an Excel mock interview.\n",
    "Your task is to generate a comprehensive, constructive, and professional feedback report for the candidate.\n",
    "\n",
    "Here are the interview results in JSON format. Each entry is for a question:\n",
    "{interview_results}\n",
    "\n",
    "The report should be clearly formatted using **Markdown** with headings and bullet points.\n",
    "Ensure the tone is encouraging yet clear about areas needing development.\n",
    "\n",
    "It must include the following sections in order:\n",
    "1.  **## Overall Performance Summary**\n",
    "    * Provide a qualitative assessment of their general Excel proficiency based on the scores across all questions.\n",
    "2.  **## Strengths**\n",
    "    * List specific Excel topics or questions where the candidate performed well (e.g., score 4 or 5), using bullet points. Summarize why they did well based on the justifications.\n",
    "3.  **## Areas for Improvement**\n",
    "    * List specific Excel topics or question types where the candidate struggled (e.g., score 1, 2, or 3), using bullet points.\n",
    "    * For each area of improvement, provide 2-3 **actionable suggestions** for learning or practice (e.g., \"Practice more with array formulas,\" \"Review VLOOKUP's lookup limitations\").\n",
    "4.  **## Topic-wise Breakdown**\n",
    "    * Provide a concise summary of performance for each unique Excel topic encountered (e.g., \"**Formulas & Functions:** Good grasp, but needs more precision on edge cases.\", \"**Pivot Tables:** Found basic creation challenging.\"). Use bullet points for each topic.\n",
    "5.  **## Overall Score & Proficiency Rating**\n",
    "    * Calculate the average score across all questions and present it (e.g., \"Average Score: X.X / 5\").\n",
    "    * Assign a general proficiency rating based on the overall performance: \"Beginner\", \"Developing\", \"Intermediate\", \"Proficient\", or \"Advanced\".\n",
    "        * Average Score 1.0-1.9: Beginner\n",
    "        * Average Score 2.0-2.9: Developing\n",
    "        * Average Score 3.0-3.9: Intermediate\n",
    "        * Average Score 4.0-4.4: Proficient\n",
    "        * Average Score 4.5-5.0: Advanced\n",
    "\"\"\"\n",
    "        )\n",
    "\n",
    "    def generate_feedback_report(self, interview_results: List[Dict[str, Any]]) -> str:\n",
    "        \"\"\"\n",
    "        Generates a comprehensive feedback report based on interview results.\n",
    "\n",
    "        Args:\n",
    "            interview_results (List[Dict[str, Any]]): A list of dictionaries,\n",
    "                                                      each containing details about a question,\n",
    "                                                      the candidate's answer, and its evaluation.\n",
    "\n",
    "        Returns:\n",
    "            str: The formatted feedback report.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert interview results to a pretty-printed JSON string for the LLM\n",
    "            # This provides the LLM with all the necessary structured data.\n",
    "            results_json_str = json.dumps(interview_results, indent=2)\n",
    "\n",
    "            # Create an LLMChain to generate the feedback report\n",
    "            # LangChainDeprecationWarning: The class `LLMChain` was deprecated... (ignored for now)\n",
    "            feedback_chain = LLMChain(llm=self.llm, prompt=self.feedback_template)\n",
    "\n",
    "            # Generate the feedback report\n",
    "            # LangChainDeprecationWarning: The method `Chain.run` was deprecated... (ignored for now)\n",
    "            report = feedback_chain.run(interview_results=results_json_str)\n",
    "            return report\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during feedback generation: {e}\")\n",
    "            return \"Failed to generate feedback report due to an internal error.\"\n",
    "\n",
    "print(\"FeedbackGenerator class defined.\")\n",
    "\n",
    "\n",
    "# --- Testing the FeedbackGenerator ---\n",
    "'''if 'llm' not in locals():\n",
    "    print(\"\\nWARNING: 'llm' not found. Please run Step 2.1 cell first.\")\n",
    "    print(\"Skipping FeedbackGenerator tests.\")\n",
    "else:\n",
    "    feedback_gen = FeedbackGenerator(llm)\n",
    "\n",
    "    # Create SAMPLE interview results to test the FeedbackGenerator\n",
    "    # This simulates the output we'd get from the ExcelEvaluator for multiple questions\n",
    "    sample_interview_results = [\n",
    "        {\n",
    "            \"id\": \"EXL001\",\n",
    "            \"topic\": \"Formulas & Functions\",\n",
    "            \"question\": \"Explain relative vs absolute cell references...\",\n",
    "            \"candidate_answer\": \"Relative moves, absolute stays.\",\n",
    "            \"evaluation\": {\"score\": 5, \"justification\": \"Candidate provided a perfectly comprehensive explanation, including partial absolute references and clear use cases. Demonstrated excellent understanding.\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"EXL002\",\n",
    "            \"topic\": \"Data Analysis & Pivot Tables\",\n",
    "            \"question\": \"Describe steps for PivotTable and purpose of Values area.\",\n",
    "            \"candidate_answer\": \"Click insert, pivot table. Values is for sums.\",\n",
    "            \"evaluation\": {\"score\": 3, \"justification\": \"Candidate understood the basic steps for PivotTable creation and the high-level purpose of the Values area. However, the explanation lacked detail on specific steps and various aggregation functions beyond sums, indicating an adequate but not comprehensive understanding.\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"EXL003\",\n",
    "            \"topic\": \"Conditional Formatting\",\n",
    "            \"question\": \"How to highlight sales above average?\",\n",
    "            \"candidate_answer\": \"Select data, home tab, conditional formatting, above average rule.\",\n",
    "            \"evaluation\": {\"score\": 4, \"justification\": \"Candidate provided accurate and concise steps for applying 'Above Average' conditional formatting. Demonstrated practical knowledge. A minor detail missing was mention of selecting the formatting style, but otherwise solid.\"}\n",
    "        },\n",
    "        {\n",
    "            \"id\": \"EXL004\",\n",
    "            \"topic\": \"VLOOKUP/XLOOKUP\",\n",
    "            \"question\": \"Explain VLOOKUP scenario, limitations, and XLOOKUP alternative.\",\n",
    "            \"candidate_answer\": \"VLOOKUP looks left to right. XLOOKUP is better.\",\n",
    "            \"evaluation\": {\"score\": 2, \"justification\": \"Candidate identified one key limitation of VLOOKUP (left-to-right) and correctly named XLOOKUP as an alternative. However, the explanation of a VLOOKUP scenario was absent, and the description of XLOOKUP's advantages was extremely brief, missing most crucial improvements. Demonstrates weak understanding.\"}\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"--- Generating Sample Feedback Report ---\")\n",
    "    print(\"=\"*50)\n",
    "    sample_report = feedback_gen.generate_feedback_report(sample_interview_results)\n",
    "    print(sample_report)\n",
    "    print(\"=\"*50 + \"\\n\")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "633e6ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MockInterviewer class defined.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'if \\'llm\\' not in locals() or \\'memory\\' not in locals() or \\'excel_questions\\' not in locals() or not excel_questions:\\n    print(\"\\nWARNING: Missing dependencies. Please run Step 1, 2, and 3 cells first.\")\\n    print(\"Cannot run Mock Interview.\")\\nelse:\\n    print(\"\\n\" + \"=\"*70)\\n    print(\"          STARTING AI EXCEL MOCK INTERVIEW SIMULATION\")\\n    print(\"=\"*70)\\n\\n    interviewer = MockInterviewer(llm, memory, excel_questions)\\n    interviewer.run_interview()\\n\\n    print(\"\\n\" + \"=\"*70)\\n    print(\"          AI EXCEL MOCK INTERVIEW SIMULATION FINISHED\")\\n    print(\"=\"*70)'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- STEP 5: Interview Logic (MockInterviewer Class) - FINAL DEFINITIVE FIX FOR CONVERSATIONAL FLOW ---\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory \n",
    "from langchain.schema import HumanMessage, AIMessage, SystemMessage \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "\n",
    "class MockInterviewer:\n",
    "    def __init__(self, llm: ChatGoogleGenerativeAI, memory: ConversationBufferMemory, questions: List[Dict[str, Any]]):\n",
    "        self.llm = llm\n",
    "        self.memory = memory \n",
    "        self.questions = questions\n",
    "        self.evaluator = ExcelEvaluator(llm)\n",
    "        self.feedback_generator = FeedbackGenerator(llm)\n",
    "        self.interview_history: List[Dict[str, Any]] = [] # To store Q&A with evaluations\n",
    "        self.current_question_index = 0\n",
    "\n",
    "        # --- File Logging Setup ---\n",
    "        timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        self.transcript_dir = \"interview_transcripts\"\n",
    "        self.feedback_dir = \"interview_feedback\"\n",
    "        os.makedirs(self.transcript_dir, exist_ok=True)\n",
    "        os.makedirs(self.feedback_dir, exist_ok=True)\n",
    "\n",
    "        self.transcript_filename = os.path.join(self.transcript_dir, f\"transcript_{timestamp}.txt\")\n",
    "        self.feedback_filename = os.path.join(self.feedback_dir, f\"feedback_{timestamp}.txt\")\n",
    "\n",
    "        self.transcript_file = open(self.transcript_filename, \"w\", encoding=\"utf-8\")\n",
    "        # --- End File Logging Setup ---\n",
    "\n",
    "        # Define a general system persona for the AI's conversational turns\n",
    "        self.system_persona = SystemMessage(\n",
    "            content=\"You are an AI-powered Excel interviewer. Your role is to conduct a structured mock interview to assess a candidate's Excel skills. Be professional, clear, and guide the candidate through the process. Do not answer Excel questions yourself, only ask them and acknowledge answers based on the provided instructions.\"\n",
    "        )\n",
    "\n",
    "    def _display_message(self, speaker: str, message: str):\n",
    "        \"\"\"Helper to print messages to console and log to file.\"\"\"\n",
    "        display_text = f\"\\n\\033[1;34mAI Interviewer:\\033[0m {message}\" if speaker == \"AI\" else f\"\\n\\033[1;32mCandidate:\\033[0m {message}\"\n",
    "        log_text = f\"\\n{speaker}: {message}\" # Plain text for log file\n",
    "\n",
    "        print(display_text)\n",
    "        if self.transcript_file:\n",
    "            self.transcript_file.write(log_text + \"\\n\")\n",
    "\n",
    "\n",
    "    def _get_ai_response(self, user_input_text: str) -> str:\n",
    "        \"\"\"Gets a conversational response from the LLM based on user input and history.\"\"\"\n",
    "        # Manually construct messages including system persona and memory content\n",
    "        messages = [self.system_persona] + self.memory.buffer_as_messages + [HumanMessage(content=user_input_text)]\n",
    "        \n",
    "        # Invoke the LLM directly\n",
    "        ai_response = self.llm.invoke(messages).content\n",
    "        \n",
    "        # Add the human and AI messages to memory for the next turn\n",
    "        self.memory.chat_memory.add_user_message(user_input_text)\n",
    "        self.memory.chat_memory.add_ai_message(ai_response)\n",
    "        \n",
    "        return ai_response\n",
    "\n",
    "    def start_interview(self):\n",
    "        \"\"\"Introduces the interviewer and explains the process.\"\"\"\n",
    "        self._display_message(\"AI\", \"Hello! I'm your AI-powered Excel Mock Interviewer.\")\n",
    "        \n",
    "        # LLM provides a single, coherent introduction\n",
    "        intro_prompt = f\"Introduce yourself as an AI Excel interviewer. Explain that you will ask {len(self.questions)} questions one by one to assess Excel proficiency. Mention that responses will be evaluated and a comprehensive feedback report will be provided at the end. Do NOT ask if the candidate is ready yet, just provide this full explanation.\"\n",
    "        \n",
    "        full_introduction = self._get_ai_response(intro_prompt)\n",
    "        self._display_message(\"AI\", full_introduction)\n",
    "\n",
    "\n",
    "    def ask_excel_question(self, question_data: Dict[str, Any], question_num: int, total_questions: int):\n",
    "        \"\"\"Asks a specific Excel question to the candidate.\"\"\"\n",
    "        self._display_message(\"AI\", f\"--- Question {question_num} of {total_questions}: {question_data['topic']} ---\")\n",
    "        \n",
    "        # LLM generates the question phrasing based on the actual question text\n",
    "        question_phrasing_prompt = f\"Please phrase the following Excel question clearly and professionally for a candidate: {question_data['question']}\"\n",
    "        llm_question_output = self.llm.invoke([self.system_persona, HumanMessage(content=question_phrasing_prompt)]).content # No memory needed for this specific phrasing\n",
    "        \n",
    "        self._display_message(\"AI\", llm_question_output)\n",
    "\n",
    "\n",
    "    def acknowledge_and_process_answer(self, candidate_answer: str):\n",
    "        \"\"\"Acknowledges the candidate's answer and prepares for next step.\"\"\"\n",
    "        # LLM acknowledges the candidate's *actual* answer\n",
    "        acknowledgement_message_prompt = f\"The candidate has just provided their answer: '{candidate_answer[:100]}...'. Please acknowledge their response briefly and professionally, without evaluating it, and indicate that you are processing it before the next question or concluding.\"\n",
    "        acknowledgement_message = self._get_ai_response(acknowledgement_message_prompt)\n",
    "        self._display_message(\"AI\", acknowledgement_message)\n",
    "\n",
    "\n",
    "    def end_interview(self):\n",
    "        \"\"\"Concludes the interview, generates feedback, and saves logs.\"\"\"\n",
    "        self._display_message(\"AI\", \"Thank you for completing the mock interview!\")\n",
    "        \n",
    "        # LLM generates final graceful conclusion\n",
    "        final_message_prompt = \"Gracefully conclude the interview. Inform the candidate that you have evaluated all their responses and are now compiling their comprehensive feedback report. Express appreciation for their time.\"\n",
    "        final_message = self._get_ai_response(final_message_prompt)\n",
    "        self._display_message(\"AI\", final_message)\n",
    "\n",
    "        self._display_message(\"AI\", \"--- Generating Your Feedback Report ---\")\n",
    "        report = self.feedback_generator.generate_feedback_report(self.interview_history)\n",
    "\n",
    "        self._display_message(\"AI\", \"--- Your Performance Feedback ---\")\n",
    "        print(report) # Still print to console for immediate viewing\n",
    "\n",
    "        # --- Save Feedback to File ---\n",
    "        try:\n",
    "            with open(self.feedback_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(report)\n",
    "            self._display_message(\"AI\", f\"Feedback report saved to: {self.feedback_filename}\")\n",
    "        except Exception as e:\n",
    "            self._display_message(\"AI\", f\"Error saving feedback report: {e}\")\n",
    "        # --- End Save Feedback ---\n",
    "\n",
    "        self._display_message(\"AI\", \"--- End of Interview ---\")\n",
    "\n",
    "\n",
    "    def run_interview(self):\n",
    "        \"\"\"Orchestrates the entire interview flow.\"\"\"\n",
    "        try:\n",
    "            self.start_interview()\n",
    "\n",
    "            self._display_message(\"AI\", \"Are you ready to start the questions now? (yes/no)\")\n",
    "            ready = input(\"Your input: \").strip().lower()\n",
    "            self._display_message(\"Candidate\", ready) # This logs the 'yes/no' response\n",
    "\n",
    "            if ready != 'yes':\n",
    "                self._display_message(\"AI\", \"No problem. We can proceed later. Goodbye!\")\n",
    "                return\n",
    "\n",
    "            # --- NEW CODE: Prompt for Name ---\n",
    "            self._display_message(\"AI\", \"Great! What is your name?\")\n",
    "            candidate_name_input = input(\"Your Name: \").strip()\n",
    "            if candidate_name_input:\n",
    "                self.candidate_name = candidate_name_input\n",
    "                self._display_message(\"Candidate\", self.candidate_name) # Log the name\n",
    "                self._display_message(\"AI\", f\"Nice to meet you, {self.candidate_name}! Let's begin.\")\n",
    "            else:\n",
    "                self.candidate_name = \"Candidate\" # Default if no name entered\n",
    "                self._display_message(\"AI\", \"No problem. We'll proceed. Let's begin.\")\n",
    "            # --- END NEW CODE ---\n",
    "\n",
    "            while self.current_question_index < len(self.questions):\n",
    "                current_question_data = self.questions[self.current_question_index]\n",
    "                question_num = self.current_question_index + 1\n",
    "                total_questions = len(self.questions)\n",
    "\n",
    "                self.ask_excel_question(current_question_data, question_num, total_questions)\n",
    "\n",
    "                candidate_answer = input(\"Your Answer: \").strip()\n",
    "                self._display_message(\"Candidate\", candidate_answer)\n",
    "\n",
    "                self.acknowledge_and_process_answer(candidate_answer)\n",
    "\n",
    "                if not candidate_answer:\n",
    "                    self._display_message(\"AI\", \"It seems you didn't provide an answer. Moving to evaluation for this question.\")\n",
    "                    candidate_answer = \"[No response provided]\"\n",
    "\n",
    "                evaluation_result = self.evaluator.evaluate_answer(\n",
    "                    excel_question=current_question_data[\"question\"],\n",
    "                    expected_answer_description=current_question_data[\"expected_answer_description\"],\n",
    "                    evaluation_criteria=current_question_data[\"evaluation_criteria\"],\n",
    "                    candidate_answer=candidate_answer\n",
    "                )\n",
    "\n",
    "                self.interview_history.append({\n",
    "                    \"id\": current_question_data[\"id\"],\n",
    "                    \"topic\": current_question_data[\"topic\"],\n",
    "                    \"question\": current_question_data[\"question\"],\n",
    "                    \"candidate_answer\": candidate_answer,\n",
    "                    \"evaluation\": evaluation_result\n",
    "                })\n",
    "                self.current_question_index += 1\n",
    "\n",
    "            self.end_interview()\n",
    "            self.memory.clear()\n",
    "\n",
    "        finally:\n",
    "            self._display_message(\"AI\", f\"Transcript saved to: {self.transcript_filename}\")\n",
    "            if self.transcript_file and not self.transcript_file.closed:\n",
    "                self.transcript_file.close()\n",
    "\n",
    "print(\"MockInterviewer class defined.\")\n",
    "\n",
    "# --- Running the Mock Interview ---\n",
    "'''if 'llm' not in locals() or 'memory' not in locals() or 'excel_questions' not in locals() or not excel_questions:\n",
    "    print(\"\\nWARNING: Missing dependencies. Please run Step 1, 2, and 3 cells first.\")\n",
    "    print(\"Cannot run Mock Interview.\")\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"          STARTING AI EXCEL MOCK INTERVIEW SIMULATION\")\n",
    "    print(\"=\"*70)\n",
    "\n",
    "    interviewer = MockInterviewer(llm, memory, excel_questions)\n",
    "    interviewer.run_interview()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"          AI EXCEL MOCK INTERVIEW SIMULATION FINISHED\")\n",
    "    print(\"=\"*70)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e91b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
